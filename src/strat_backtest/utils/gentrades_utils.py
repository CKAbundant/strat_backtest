"""Utility functions used in 'GenTrades' class."""

from datetime import datetime
from decimal import Decimal
from pathlib import Path
from typing import Any

import pandas as pd

from strat_backtest.utils.constants import ExitMethod
from strat_backtest.utils.utils import correct_datatype


def get_module_paths(main_pkg: str = "strat_backtest") -> dict[str, str]:
    """Convert file path to package path that can be used as input to importlib.

    Args:
        script_path (str):
            Relative path to python script containig required module.
        main_pkg (str):
            Name of main package to generate module path
            (Default: "strat_backtest").

    Returns:
        module_info (dict[str, str]):
            Dictionary mapping each concrete class to module path.
    """

    # Get main package directory path
    main_pkg_path = Path(__file__).parents[1]

    # Get list of folder paths containin concrete implementation of 'EntryStruct',
    # 'ExitStruct', 'StopLoss' and 'TrailProfit' abstract class.
    folder_paths = [
        rel_path
        for rel_path in main_pkg_path.iterdir()
        if rel_path.is_dir() and rel_path.name not in {"base", "utils", "__pycache__"}
    ]

    module_info = {}

    # Iterate through contents of each folder path
    for folder in folder_paths:
        # Get all python scripts inside folder
        for file_path in folder.glob("*.py"):
            file_name = file_path.stem
            folder_name = folder.stem

            # Ignore __init__.py
            if file_name == "__init__":
                continue

            # Get name of concrete class
            class_name = "".join(
                part.upper() if part in {"fifo", "lifo"} else part.title()
                for part in file_name.split("_")
            )

            module_info[class_name] = f"{main_pkg}.{folder_name}.{file_name}"

    return module_info


def append_info(
    df_signals: pd.DataFrame,
    info_list: list[dict[str, str | datetime | Decimal]] | None = None,
) -> pd.DataFrame:
    """Convert 'info_list' (i.e. stop loss or trailing profit info) to DataFrame
    and append to 'df_signals'."""

    if len(info_list) == 0:
        return df_signals

    # Convert 'info_list' to DataFrame
    df_info = pd.DataFrame(info_list)

    # Ensure both 'date' column in 'df' and 'df_info' are time zone naive
    df_signals = set_naive_tz(df_signals)
    df_info = set_naive_tz(df_info)

    # Perform join via index
    df = df_signals.join(df_info)

    # Convert 'date' index to column
    df = df.reset_index()

    return df


def gen_mapping(record: tuple[Any], req_cols: list[str]) -> dict[str, Any]:
    """Generate mapping for record generated by 'itertuples' method given
    list of required columns."""

    # Record include row index and required fields
    fields = ["idx", *req_cols]

    # Create dictionary by matching column names to respective fields
    record = dict(zip(fields, record))

    # Ensure numeric type are set to Decimal and date type are set to datetime
    record = correct_datatype(record)

    return record


def set_naive_tz(data: pd.DataFrame) -> pd.DataFrame:
    """Set the date type column to be time zone naive and as index to
    faciliate join."""

    # Check for columns contain date type records
    date_cols = get_date_cols(data)

    if not date_cols:
        raise ValueError("No columns contain date objects found.")

    if len(date_cols) > 1:
        raise ValueError(
            f"DataFrame contains more than 1 date type column i.e. {date_cols}"
        )

    # 'date_col' should contain only 1 item i.e. only 1 date type column
    col = date_cols[0]

    # Set date type column to timezone naive
    df = data.copy()
    df[col] = pd.to_datetime(df[col])
    df[col] = df[col].map(lambda dt: dt.replace(hour=0, minute=0, tzinfo=None))
    df = df.set_index(col)

    return df


def get_date_cols(df: pd.DataFrame) -> list[str]:
    """Get list of columns that are of date type i.e.
    datetime object or Pandas timestamp object."""

    date_cols = []

    for col in df.columns:
        # Get list of unique data types for each column
        datatype_set = {type(rec) for rec in df[col]}

        # Check if record in columns are either datetime or pd.Timestamp object
        if datatype_set & {pd.Timestamp, datetime}:
            date_cols.append(col)

    return date_cols


def validate_req_cols(
    df: pd.DataFrame, req_cols: list[str], exit_struct: ExitMethod
) -> pd.DataFrame:
    """Ensure all columns in 'self.req_cols' are present in DataFrame; and return
    filtered DataFrame based on 'self.req_cols' columns.

    Args:
        df (pd.DataFrame): DataFrame containing OHLCV and other relevant info.
        req_cols (list[str]): list of required columns.
        exit_struct (ExitMethod): Exit strategy e.g. 'FixedExit', etc.

    Returns:
        (pd.DataFrame): Validated DataFrame with required columns.
    """

    # 'profit' and 'stop' columns are required if exit_struct is 'FixedExit'
    if exit_struct == "FixedExit":
        req_cols.extend(["stop"])

    not_available = [col for col in req_cols if col not in df.columns]

    if not_available:
        raise ValueError(f"{not_available} required columns are missing!")

    return df.loc[:, req_cols]
